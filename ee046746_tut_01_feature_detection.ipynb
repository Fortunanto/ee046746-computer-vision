{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "\n",
    "# Tutorial 01 - Feature Detection\n",
    "---\n",
    "<img src=\"./assets/LineDetection.png\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://towardsdatascience.com/tutorial-build-a-lane-detector-679fd8953132\">Image source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "\n",
    "* Reminder - Derivatives and Edges\n",
    "    * Image Gradient\n",
    "* Corner Detection\n",
    "    * Why Corners?\n",
    "    * Harris Corner Detector\n",
    "    * Harris Conclusion With Vectorized Operations\n",
    "    * Invariance to Image Deformation\n",
    "    * Laplacian Feature Detector\n",
    "* Multi-scale Detection\n",
    "    * Multi-scale Laplacian\n",
    "    * Multi-scale With Harris\n",
    "* Recommended Videos\n",
    "* Credits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Reminder - Derivatives and Edges\n",
    "\n",
    "---\n",
    "* An edge is a place of rapid change in the image intensity function.\n",
    "\n",
    "<img src=\"./assets/tutorial_1/derive1.JPG\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Image Gradient\n",
    "---\n",
    "* The gradient of an image: $\\nabla f= [\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}]$\n",
    "* The gradient points in the direction of most rapid change in intensity \n",
    "\n",
    "<img src=\"./assets/derive2.JPG\" width=\"600\">\n",
    "\n",
    "* The gradient direction (orientation of edge normal) is given by: $\\theta = tan^{-1} (\\frac{\\partial f}{\\partial y}/\\frac{\\partial f}{\\partial x})$\n",
    "\n",
    "* The edge strength is given by the gradient magnitude\n",
    "$||\\nabla f|| = \\sqrt {(\\frac{\\partial f}{\\partial y})^2+(\\frac{\\partial f}{\\partial x})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Image derivatives can be implemented as a convolution:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/image_derivatives.png\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Corner Detection\n",
    "\n",
    "Although edges are good for human perception, and contain most of the information, they are not allways good enough. Another, more useful building block for many computer vision applications is a corner detector. Corners are better features for tasks such as:\n",
    "* Image alignment (homography, fundamental matrix)\n",
    "* 3D reconstruction\n",
    "* Motion tracking\n",
    "* Object recognition\n",
    "* Indexing and database retrieval\n",
    "* Robot navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./assets/tutorial_1/planar_instance_recognition.png\" style=\"height:300px\"> <img src=\"./assets/tutorial_1/image_matching.png\" style=\"height:300px\">\n",
    "\n",
    "<img src=\"./assets/tutorial_1/3d_obj_recognition.png\" style=\"height:300px\"> <img src=\"./assets/tutorial_1/robot_localization.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Why Corners?\n",
    "\n",
    "suppose we select the yellow window, which contains an edge, as a feature. we want to find it in an image from a different angle:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/grafity_bad_feature.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Where is the match?\n",
    "\n",
    "<img src=\"./assets/tutorial_1/grafity_bad_feature2.png\" style=\"height:300px\">\n",
    "\n",
    "There are multiple windows with similar content, just by looking at the local content we won’t be able to tell which one matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's see another example. Pick a point for a feature in the following image:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/Trapez.png\" style=\"height:200px\">\n",
    "\n",
    "Now find it again in the next image:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/Trapez_rotated.png\" style=\"height:200px\">\n",
    "\n",
    "What type of feature would be a good selection? Corners of course!\n",
    "Corners are easily recognised by looking at a small window. Shifting the window in any direction should give large change in intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./assets/tutorial_1/flat_edge_corner.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Harris Corner Detector\n",
    "\n",
    "Given a window $W$ of $nxn$ pixels, we perform the following steps to determine whether it’s a corner:\n",
    "1.\tCompute image gradients over small region\n",
    "2.\tSubtract the mean from each image gradient\n",
    "3.\tCompute the covariance matrix\n",
    "4.\tCompute eigenvectors and eigenvalues\n",
    "5.\tUse threshold on eigenvalues to detect corners\n",
    "\n",
    "We slide a window through the image and perform all of these steps for each window.\n",
    "We will discuss each step separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1. Compute image gradients over a small region\n",
    "\n",
    "For our window $W$, we compute the derivative in each direction (x and y), which generates two $nxn$ arrays $W_x, W_y$\n",
    "\n",
    "<img src=\"./assets/tutorial_1/window_derivatives.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Recall that we get high response in $W_x, W_y$ if there is an edge on the corresponding direction. \n",
    "\n",
    "<img src=\"./assets/tutorial_1/derivatives_example.png\" style=\"height:300px\">\n",
    "\n",
    "*Note: Since we are going to perform this process on a sliding window, we are going to need to compute the gradient at all points any way.In practice it will be more efficient to compute the gradients for the entire image and extract the current window from the derivatives image.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "For each pixel $i,j$ in the window, we have a gradient vector $(W_x^{i,j}, W_y^{i,j})$. We would like to look at the distribution of these gradients for the window. We can plot the $n^2$ gradients on a 2d plane, let's see some examples:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/grad_distribution.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "on the left pach, we have a flat region, therefore the derivative in both directions are small, and most of the points will be around the origin (assume there is some noise, that's why gradients are not exactly zero)\n",
    "\n",
    "on the central patch, we have a vertical edge, therefore the $x$ derivative is high on some points, we get a distribution around the x axis.\n",
    "\n",
    "on the right patch, there are some pixels with vertical only gradient and some pixels with a diagonal gradient, so we get points spread along two lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "2D plot are nice, but we want a way to quantify the results and determine if there's a corner in the patch. We are going to do that by looking at the covariance matrix of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2. Subtract the mean from each image gradient\n",
    "To define the covariance we first compute the mean of the data and subtract it, so that the data is centered around 0. \n",
    "<img src=\"./assets/tutorial_1/subtract_mean.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3. Compute the covariance matrix\n",
    "For a general distribution, given a set of $N$ 2D vectors   $v^1,v^2,…$ (after mean subtraction) the covariance is defined as\n",
    "\n",
    "$$\\Sigma = \\frac{1}{N} \\sum_{j=1}^{N} v^j {v^j}^T \n",
    " = \\frac{1}{N} \\begin{bmatrix}\n",
    " \\sum_{j=1}^{N} v^j_x v^j_x  & \\sum_{j=1}^{N} v^j_x v^j_y \\\\\n",
    "\\sum_{j=1}^{N} v^j_x v^j_y & \\sum_{j=1}^{N} v^j_y v^j_y\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note that there are 3 sums in the covariance matrix. In our case this covariance matrix is:\n",
    "\n",
    "$$\\Sigma = \\frac{1}{N} \\sum_{i,j \\in 1...n} g^j {g^j}^T\n",
    " = \\frac{1}{n^2} \\begin{bmatrix}\n",
    " \\sum_{i,j \\in 1...n} {W_x^{i,j}}^2 & \\sum_{i,j \\in 1...n} W_x^{i,j}W_y^{i,j}\\\\\n",
    "\\sum_{i,j \\in 1...n} W_x^{i,j}W_y^{i,j} & \\sum_{i,j \\in 1...n} {W_y^{i,j}}^2\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The sums can be computed the following way:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/cov_matrix_sums.png\" style=\"width:600px\">\n",
    "\n",
    "where .* means matrix elementwise multipication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 4. Compute Eigenvalues \n",
    "\n",
    "#### Reminder\n",
    "Since the covariance matrix is symetric (and PSD = Positive Semi Defined), it can be decomposed:\n",
    "$$\\Sigma = R^T \\Lambda R = \n",
    "\\begin{bmatrix}\n",
    "-\\bar{r_1}^T- \\\\\n",
    "-\\bar{r_2}^T-\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1  & 0\\\\\n",
    "0 & \\lambda_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\bar{r_1} & \\bar{r_2} \\\\ \n",
    "| & |\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For a symetric PSD matrix this is called SVD - Singular Value Decomposition. \n",
    "Where $\\lambda_1$ and $\\lambda_2$ are the eigenvalues, and R is an orthogonal matrix (can say it's a rotation matrix), built from orthogonal eigenvectors. \n",
    "\n",
    "Note that in this decomposition $R^{-1} = R^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In a 2D covariance matrix, the eigenvectors point to the directions with the highest and lowest variance. The eigenvalues are the corresponding variances.\n",
    "<img src=\"./assets/tutorial_1/covariance_eigens.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Back to corners\n",
    "In the decomposition $\\Sigma = R^T \\Lambda R $ the eigenvectors (columns of R) are the directions where there is maximal and minimal variance in the gradients of the window, and the corresponding eigenvalues are the variance. That means that if we rotate the patch, only R will change, but the eigenvalues will remain the same.\n",
    "\n",
    "We want to use the eigenvalues for corner detection, this will also promise us that our detector will be invariant to rotations. \n",
    "\n",
    "<img src=\"./assets/tutorial_1/corner_rotation.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In a corner, both eigenvalues should be high\n",
    "\n",
    "<img src=\"./assets/tutorial_1/corner_edge_variance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 5. Use threshold on eigenvalues to detect corners\n",
    "\n",
    "We have the eigenvalues, and we want to decide whether our patch is a corner. Where do we want $(\\lambda_1, \\lambda_2)$ to be?\n",
    "\n",
    "<img src=\"./assets/tutorial_1/lambdas_plane.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "* In a flat window, we have low variance in any direction, which means both lambda's will be close to zero.\n",
    "* In an edge, we have high variance on one direction but low variance on the orthogonal direction, resulting one lambda high and one low.\n",
    "* In a corner, we have high variance in two directions (our ellipse will become a large circle), both lambda will be high. \n",
    "\n",
    "<img src=\"./assets/tutorial_1/lambdas_plane_with_patches.png\" style=\"height:300px\">\n",
    "\n",
    "**Clarification: we can get $\\lambda_1<<\\lambda_2$ or $\\lambda_2<<\\lambda_1$ in a non vertical or horizontal edge as well. If we have a diagonal edge, one eigenvalue will represent the variance on it's direction and one on the orthogonal direction. This is due to how the SVD works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Can you think of a good score function $R(\\lambda_1, \\lambda_2)$ for a window?\n",
    "\n",
    "<img src=\"./assets/tutorial_1/lambdas_strong_corner.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "One such a function we can use is the min eigenvalue. If it is high then we have a corner. \n",
    "$$ R = min(\\lambda_1, \\lambda_2)$$\n",
    "\n",
    "<img src=\"./assets/tutorial_1/lambdas_min.png\" style=\"height:300px\">\n",
    "\n",
    "*Note: In some formulations the SVD decompoziation is defined such that the eigenvalues are in descending order, this way we can have threshold only on $\\lambda_2$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The problem with the minimum function, is that we need to know the eigenvalues. performing the computation is quite expensive. Recall that we have to do it many times on a sliding window. We can use some tricks to find a cheaper score function.\n",
    "\n",
    "Recall that the sum of eignevalues of matrix equals to it's trace, and their multipications to it's determinant. for 2x2 matrix:\n",
    "$$\\Sigma = \\begin{bmatrix} \\sigma_1 &\\sigma_2 \\\\ \\sigma_3 & \\sigma_4 \\end{bmatrix}$$\n",
    "\n",
    "$$ det(\\Sigma) = \\lambda_1 \\lambda_2 = \\sigma_1 \\sigma_4 - \\sigma_2 \\sigma_3 \\\\\n",
    "trace(\\Sigma) = \\lambda_1 + \\lambda_2 = \\sigma_1 + \\sigma_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We don't have the eigenvalues, but we have their multipication and sum. we can define another, less exprensive to compute, score function:\n",
    "$$R = \\lambda_1 \\lambda_2 - \\kappa (\\lambda_1 + \\lambda_2)^2 \\\\ \\kappa < 0.25$$\n",
    "\n",
    "Let's look at the function, intuitively\n",
    "* If $\\lambda_1=\\lambda_2=0$ we get R = 0\n",
    "* If $\\lambda_1>0, \\lambda_2=0$, then $\\lambda_1 \\lambda_2=0, (\\lambda_1+\\lambda_2)^2>0 $ we get R<0\n",
    "* If $\\lambda_1=\\lambda_2>0$ we get $R = \\lambda_1^2 (1-4\\kappa) > 0 $\n",
    "\n",
    "So this score is high when both eigenvalues are high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here is an example with $\\kappa = 0.2$ and different thresholds for the score:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/desmos_scores.png\" style=\"height:500px\">\n",
    "\n",
    "https://www.desmos.com/calculator/oi9q42yvt4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Harris Conclusion With Vectorized Operations\n",
    "When working with images we want as match as possible to define our operations with matrices. This is a more efficient way to implement with high level programming languages, and this is the only way to make use of computation accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's conclude the Harris Detector, with more efficient vector operations. Given an image $I$:\n",
    "\n",
    "#### 1\n",
    "Compute x and y derivatives $$I_x = G^x * I$$ $$I_y = G^y * I$$ $G^x$ and $G^y$ are gaussian derivatives filters, which perform the blur and derivation simultaneously.\n",
    "\n",
    "#### 2\n",
    "\n",
    "compute the product of the derivatives for every pixel $$I_{x^2} = I_x \\cdot I_x $$ $$I_{y^2} = I_y \\cdot I_y $$  $$I_{xy} = I_x \\cdot I_y $$\n",
    "(we use elementwise product here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### 3\n",
    "compute the sum of the products for a window around each pixel. That can also be done with convolution with a gaussian blur filter to remove noise.\n",
    "    $$ S_{x^2} = G_\\sigma * I_{x^2}$$\n",
    "    $$ S_{y^2} = G_\\sigma * I_{y^2}$$\n",
    "    $$ S_{xy} = G_\\sigma * I_{xy}$$\n",
    "#### 4\n",
    "Compute the score for each window. The score is defined by the eigenvalues of the covariance matrix:\n",
    "    $$\\Sigma(x,y) = \\begin{bmatrix} S_{x^2}(x,y) & S_{xy}(x,y) \\\\ S_{xy}(x,y) & S_{y^2}(x,y) \\end{bmatrix}$$\n",
    "    but we don't need to compute it for the score, just the determinant and trace for each x,y. We can do it with one vectorized operation for all  pixels. $ \\overline{\\overline{R}} $ will be a 2d image with the score for each pixel (and the window around it). \n",
    "    $$\\overline{\\overline{R}} = det(\\Sigma) - \\kappa(trace(\\Sigma))^2 = S_{x^2}S_{y^2} - S_{xy}^2 - \\kappa (S_{x^2} + S_{y^2})^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### 5\n",
    "threshold on value of R to determine where are the corners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Invariance to Image Deformation\n",
    "\n",
    "Recall that we want to detect the same corner features in different images of the same object. These images may undergo various deformations:\n",
    "* Change in view point\n",
    "* Zoom (scale)\n",
    "* Illumination\n",
    "* Rotation\n",
    "* ...\n",
    "\n",
    "We already know that Harris Detector is invariant to  rotation, since eigenvalues of the covariance matrix dont change under rotation, just the rotation matrix (the eigenvectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What about illumination? Illumination usually is descrived as affine intensity change. \n",
    "\n",
    "for an image $I(x,y)$ we could define such transformation with parameters $a,b$ as $I'(x,y) = aI(x,y) + b$.\n",
    "\n",
    "Since Harris uses derivatives of the image, it won't be affected by the constant intensity change b, but it's not invariant to the change in scale caused by a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What about scale? Harris is **not** invariant to scale!\n",
    "\n",
    "<img src=\"./assets/tutorial_1/scale_variance.png\" style=\"height:200px\">\n",
    "\n",
    "We will soon see what can we do about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Laplacian Feature Detector\n",
    "Ther laplacian of an image is a version of a \"scalar multidimensional second derivative\". Given an image (or a window) $I(x,y)$ the laplacian is defined as:\n",
    "$$ L(I) = \\frac{\\partial^2 I}{\\partial x^2} + \\frac{\\partial^2 I}{\\partial y^2}$$ \n",
    "\n",
    "Laplacian filter is often combined with gaussian filter to blur the image. Applying gaussian blur and then laplacian on an image, is equivalent to applying the laplacian on the gaussian, and then convolve the resulted filter on the image. This way we get the LoG - Laplacian of Gaussian filter.\n",
    "\n",
    "<img src=\"./assets/tutorial_1/LoG.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Instead of computing the second derivative of the gaussian, it is simpler and more efficient (we will see why soon) to just subtract two gaussians with different variances to approximate it.\n",
    "\n",
    "<img src=\"./assets/tutorial_1/DoG.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "This filter have high response for blobby circular image areas. \n",
    "\n",
    "<img src=\"./assets/tutorial_1/sunflowers_laplacian.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The laplacian, like the Harris Detector, is affected by scale. a laplacian filter with different sigmas will have high response for different patches depending on their scale. Note the difference for several variances, on the same image with different scale:\n",
    "\n",
    "<img src=\"./assets/tutorial_1/sunflowers_response1.png\" style=\"width:400px\"> <img src=\"./assets/tutorial_1/sunflowers_response2.png\" style=\"width:400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./assets/tutorial_1/sunflowers_response3.png\" style=\"width:400px\"> <img src=\"./assets/tutorial_1/sunflowers_response4.png\" style=\"width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./assets/tutorial_1/sunflowers_response5.png\" style=\"width:400px\"> <img src=\"./assets/tutorial_1/sunflowers_response6.png\" style=\"width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Response is highest when signal has the same characteristic scale as the filter.\n",
    "\n",
    "<img src=\"./assets/tutorial_1/laplacian_response.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Multi-scale Detection\n",
    "We are going to see multi-scale detection on Laplacian (DoG) feature detector. Same ideas can be applied on the Harris Detector, but the laplacian is a bit easier to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Multi-scale Laplacian\n",
    "How can we make a feature detector scale-invariant?\n",
    "We can look for features in multiple scales, and find local maxima in both position and scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To compute multiple laplacians for an image, we first compute gaussians with different scales (variances), and then subtract each gaussian image from it's neighbor. We get multiple laplacians images with different scales. This is called DoG pyramid.\n",
    "\n",
    "<img src=\"./assets/tutorial_1/DoG_pyramid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "In normal Laplacian Detector, we would have chosen features at points with local maximal response to the filter in space (x and y).\n",
    "\n",
    "In multi-scale laplacian, we will choose maximal response in space and scale (x,y and scale):\n",
    "\n",
    "<img src=\"./assets/tutorial_1/multi_scale_max.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./assets/tutorial_1/sunflower_max_response.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Filtering Edge Features\n",
    "Laplacian filter has high response to circles, but it also has quite a high response for straight lines and edges, which we don't want as features, although they are local maximum points.  \n",
    "\n",
    "<img src=\"./assets/tutorial_1/butterfly_features.png\">\n",
    "\n",
    "Detection is usually followed by cleaning stage to maintain only informative features. Usually using techniques similar to those we saw in Harris Detector: look at eigenvalues of the covariance matrix of local derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/edge-level.png\" style=\"height:50px;display:inline\"> Multi-scale With Harris\n",
    "\n",
    "We have seen multi-scale detection of Laplacian (difference of Gaussian) features. \n",
    "One can also define a Harris corner detector in a multi-scale way: change the scale of the derivative filter and change the size of the window over which we average gradients.  We can then look for local extrema in the scale space.\n",
    "For various historical reasons, the popular SIFT descriptor you will see in the lecture detects keypoints using difference of Gaussians rather than Harris.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "* Edge Detection (Sobel + Canny) -  <a href=\"https://www.youtube.com/watch?v=07qwMn2c5iY\"> CSCI 512 - Lecture 09-1 Edge Detection </a> | <a href=\"https://www.youtube.com/watch?v=PUcbhaGhgtE\"> CSCI 512 - Lecture 09-2 Edge Detection </a>\n",
    "    * Edge Detection (Sobel) - <a href=\"https://www.youtube.com/watch?v=uihBwtPIBxM\"> Finding the Edges (Sobel Operator) - Computerphile </a>\n",
    "    * Edge Detection (Canny) - <a href=\"https://www.youtube.com/watch?v=sRFM5IEqR2w\"> Canny Edge Detector - Computerphile </a>- \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "----\n",
    "* Slides - Anat Levin\n",
    "* EE 046746 Spring 2020 - <a href=\"https://il.linkedin.com/in/dahlia-urbach-97a816123\">Dahlia Urbach</a>\n",
    "* Tutorial: Build a lane detector - <a href=\"https://towardsdatascience.com/tutorial-build-a-lane-detector-679fd8953132\">Chuan-en Lin</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6da3a9e59b620b8cdab8cbe2817e8ae0358faefd13fe5d838bbf9f636ebda883"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}