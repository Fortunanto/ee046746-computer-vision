{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "---\n",
    "#### Hila Manor\n",
    "\n",
    "\n",
    "## Tutorial 12 - Attention and Transformers\n",
    "---\n",
    "\n",
    "### Prompt: university students learning about transformers and attention, you can clearly see a self-attention title in the slides\n",
    "\n",
    "<img src=\"./assets/attn_stablediff.jpeg\" style=\"height:300px\">\n",
    "\n",
    "* <a href=\"https://stablediffusionweb.com/#demo/\">Do it yourselves</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Processing Sequences](#-Processing-Sequences)\n",
    "* [Attention](#-Attention)\n",
    "* [Self-Attention](#-Self-Attention)\n",
    "* [Who are Q, K, V?](#-Who-are-Q,-K,-V?)\n",
    "* [Multiple Heads](#-Multiple-Heads)\n",
    "* [The Transformer](#-The-Transformer)\n",
    " * [Architecture Breakdown](#-Let's-Break-it-Down)\n",
    "* [A Zoo of Transformers](#-A-Zoo-of-Transformers)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/100/000000/workflow.png\" style=\"height:50px;display:inline\"> Processing Sequences\n",
    "---\n",
    "\n",
    "* Many domains have data that is sequential in nature.\n",
    " * Natural language processing (NLP), speech and audio processing, financial data (such as stock prices), etc.\n",
    "* In sequential data, each data point depends on the ones that come before it.\n",
    "<img src=\"./assets/attn_cpc_seq.png\">\n",
    "* So we want **context**.\n",
    "  * A 5 that came after a 4, which came after a 3, and so on...\n",
    "  * But how long should our context be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* It turns out that \"forever\" is too long of a time period (Go figure).\n",
    "<img src=\"./assets/attn_inf_context.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Do we actually need \"forever?\"\n",
    "\n",
    "* **Definition of Computer Vision** from Wikipedia:\n",
    "  * Computer vision is an interdisciplinary field that deals with how computers can be made to gain <mark>high-level understanding from digital images or videos</mark>. From the perspective of engineering, it seeks to <mark>automate tasks</mark> that the <mark>human visual system</mark> can do. \"Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the <mark>development of a theoretical and algorithmic basis</mark> to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/50/000000/whistle.png\" style=\"height:50px;display:inline\"> Attention\n",
    "---\n",
    "\n",
    "* Lets teach a model to **pay attention** to the more important features and understand the relationships between them.\n",
    "* An example:\n",
    "  * Task: Translate the English sentence \"I am a student\" to French.\n",
    "  * Let's assume we have a dictionary where each English word (a **key**) has a direct translation to French (a **value**)\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Keys</th>\n",
    "<th>Values</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody><tr>\n",
    "<td>student</td>\n",
    "<td>étudiant</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>I</td>\n",
    "<td>je</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>...</td>\n",
    "<td>...</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>painting</td>\n",
    "<td>tableau</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "  * To translate the English query: \"I am a student\", we can look up the translations for each word (a **query**) in the dictionary.\n",
    "* Will every key have a unique translation?\n",
    "  * The correct translation is \"Je suis étudiant\". Only 3 words!\n",
    "  * What is the role of the word \"a\" in this context?\n",
    "* Maybe the translation depends on more than just one word?\n",
    "  * I am <mark>a female student</mark> -> Je suis <mark>étudiante</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Formally:\n",
    "  * **Query** - The input we now want to search in our database: $\\mathbf{q}_i \\in \\mathbb{R}^{d_q}$\n",
    "    * $i \\in \\{1,...,n\\}$, where $n$ is the length of our query.\n",
    "  * **Key** - The entries in our database: $\\mathbf{k}_j \\in \\mathbb{R}^{d_k}$\n",
    "    * They're from the same \"family\" so usually $d_q=d_k$    \n",
    "  * **Value** - A data-point in the destination domain we're searching for- $\\mathbf{v}_j \\in \\mathbb{R}^{d_v}$\n",
    "    * $j \\in \\{1,...,m\\}$, where $m$ is the size of our database.\n",
    "\n",
    "\n",
    "* To query our database, we need to find the key in the dictionary that is the most similar to our query.\n",
    "  * The most common similarity function is the scaled dot product:\n",
    "   $$similarity(\\mathbf{q}_i, \\mathbf{k}_j) = \\frac{\\mathbf{q}_i^T\\mathbf{k}_j}{\\sqrt{d_k}}$$\n",
    "* Then we weigh the result according to how simliar the key was to the original query:\n",
    "   $$Attention(\\mathbf{q}_i,\\{\\mathbf{k}_j\\}_{j=1}^m,\\{\\mathbf{v}_j\\}_{j=1}^m) = \\sum_j similarity(\\mathbf{q}_i, \\mathbf{k}_j)\\cdot \\mathbf{v}_j = \\sum_j \\frac{\\mathbf{q}_i^T\\mathbf{k}_j}{\\sqrt{d_k}}\\cdot \\mathbf{v}_j$$\n",
    "\n",
    "* We can stack keys-vectors on top of each other ($\\mathbf{K} \\in \\mathbb{R}^{d_k\\times m}$) to get all of the similarity values at once for a single query: $\\frac{\\mathbf{q}_i^T\\mathbf{K}}{\\sqrt{d_k}}$\n",
    "  * We can add a softmax on top to normalize it: $softmax\\left(\\frac{\\mathbf{q}_i^T\\mathbf{K}}{\\sqrt{d_k}}\\right)$\n",
    "* We can stack values similarly ($\\mathbf{V} \\in \\mathbb{R}^{d_v\\times m}$) to weigh all of the values at once: $softmax\\left(\\frac{\\mathbf{q}_i^T\\mathbf{K}}{\\sqrt{d_k}}\\right)\\mathbf{V}$\n",
    "* And we can also stack the queries ($\\mathbf{Q} \\in \\mathbb{R}^{d_q\\times n}$) to query everything at once:\n",
    "$$Attention(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) = softmax\\left(\\frac{\\mathbf{Q}^T\\mathbf{K}}{\\sqrt{d_k}}\\right)\\mathbf{V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/attn_jay_attention.png\" style=\"height:500px;\" /></center>\n",
    "\n",
    "* <a href=\"https://jalammar.github.io/illustrated-transformer/\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Attention Weights Matrix Visualization\n",
    "---\n",
    "<center><img src=\"./assets/attn_attnmat.png\" style=\"height:500px\"\"></center>\n",
    "\n",
    "* [Image Source: D. Bahdanau et al](https://arxiv.org/pdf/1409.0473.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A famous visualization example of attention from an image-captioning paper ([Xu et al. 2015](http://proceedings.mlr.press/v37/xuc15.pdf)):\n",
    "<center><img src=\"./assets/attn_xu2015_1.png\" width=\"1500\" /></center>\n",
    "\n",
    "* [Image Source: K. Xu et al.](http://proceedings.mlr.press/v37/xuc15.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/50/000000/signal-horn.png\" style=\"height:50px;display:inline\"> Self-Attention\n",
    "---\n",
    "* What if we want to find relationships between elements in the same input?\n",
    "  * If it looks like a car, sounds like a car, but is on water... Then it's a boat.\n",
    "<center><img src=\"./assets/attn_carboat.jpg\" style=\"height:400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* in self-attention we only have one domain.\n",
    "* So we use the same \"query\" input for the keys as well\n",
    " * We query it against itself.\n",
    "* And the destination domain also uses the same input (from English to English, to English again).\n",
    "\n",
    " <center><img src=\"./assets/attn_self_att_words.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/ask-question.png\" style=\"height:50px;display:inline\"> Who are Q, K, V?\n",
    "---\n",
    "\n",
    "* How did we get from a word to these vectors? \n",
    "  * How do we get from **an image** to these vectors?\n",
    "* Cut the image to 16x16 patches, and *embed* each patch using an linear projection head (MLP).\n",
    "  * Add some form of positional encoding, so we will have some sense of \"where\" each patch was.\n",
    "  <center><img src=\"./assets/attn_vit_emb.gif\" style=\"width:800px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Now to create $\\mathbf{Q},\\mathbf{K},\\mathbf{V}$, we multiply the embedded input $\\mathbf{x}_i\\in \\mathbb{R}^{d_{emb}}$ y **learnable** matrices: $\\mathbf{W_Q}\\in \\mathbb{R}^{d_q \\times d_{emb}}, \\mathbf{W_K}\\in \\mathbb{R}^{d_k \\times d_{emb}}, \\mathbf{W_V}\\in \\mathbb{R}^{d_v \\times d_{emb}}$\n",
    "$$\\mathbf{q}_i = \\mathbf{W_Q}\\mathbf{x}_i$$\n",
    "* In self-attention, the same input is used for the keys and values, remember?\n",
    "$$\\mathbf{k}_i = \\mathbf{W_K}\\mathbf{x}_i$$\n",
    "$$\\mathbf{v}_i = \\mathbf{W_V}\\mathbf{x}_i$$\n",
    "<center><img src=\"./assets/attn_jay_emb.png\" style=\"width:800px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/50/000000/user-group-man-man.png\" style=\"height:50px;display:inline\"> Multiple Heads\n",
    "---\n",
    "\n",
    "* Can we pay attention to different stuff for the same input?\n",
    "<center><img src=\"./assets/attn_jay_multihead.png\"></center>\n",
    "\n",
    "* [Tensor2Tensor Notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Learn multiple $\\mathbf{W_Q}\\, \\mathbf{W_K}, \\mathbf{W_V}$ matrices, and combine the results from the different \"heads\".\n",
    "\n",
    "<img src=\"./assets/attn_jay_multiple_head_attn.png\" width=900 /> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/50/000000/transformer.png\" style=\"height:50px;display:inline\"> The Transformer\n",
    "---\n",
    "* The attention scheme received a lot of attention thanks to the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (+61642 citations).\n",
    "  * Before it, sequential data was processed... Sequentially.\n",
    "    * Can't maximize the utilisation of the GPU! \n",
    "  * Using attention, the data can be processed **all-at-once**.\n",
    "    * Using the parallelization capabilities of GPUs.\n",
    "* The transformer is composed of an **Encoder** and a **Decoder**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/dusk/50/000000/overview-pages-2.png\" style=\"height:50px;display:inline\"> Encoder-decoder architectures\n",
    "---\n",
    "A common type of architecture used in many tasks.\n",
    "\n",
    "* The **encoder** maps the input to some latent representation, usually of a low dimension.\n",
    "  * In the context of our course - the encoder is some type of **feature extractor**\n",
    "* The **decoder** applies a different mapping, from the latent space to some other space (sometimes back to the input space).\n",
    "  * This can be used to *generate* **new** data.\n",
    "\n",
    "<center><img src=\"./assets/attn_enc_dec.png\" width=400 /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-those-icons-fill-those-icons/50/000000/external-Decepticon-geek-those-icons-fill-those-icons.png\" style=\"height:50px;display:inline\">  Scary Transformer Diagram\n",
    "---\n",
    "\n",
    "<center><img src=\"./assets/attn_transformer.png\" style=\"height:450px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-flaticons-lineal-color-flat-icons/50/000000/external-building-parts-edutainment-flaticons-lineal-color-flat-icons.png\" style=\"height:50px;display:inline\"> Let's Break it Down\n",
    "---\n",
    "* Let's start from what we know:\n",
    "<center><img src=\"./assets/attn_transformer_attn.png\" style=\"height:400px;\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"./assets/attn_transformer_multihead.png\" style=\"height:400px;\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* In the transformer:\n",
    "  * The encoder uses attention layers to create meaningful features.\n",
    "  * The decoder uses the previous outputs to query which of those features is important for the next output (keys and values).\n",
    "\n",
    "<center><img src=\"./assets/attn_transformer_encdec.png\" style=\"height:450px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* In reality we stack encoders and decoders:\n",
    "  \n",
    "<center><img src=\"./assets/attn_transformer_stacked.png\" style=\"height:300px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-flaticons-lineal-color-flat-icons/50/000000/external-zoo-summer-travel-flaticons-lineal-color-flat-icons-2.png\" style=\"height:50px;display:inline\"> A Zoo of Transformers\n",
    "---\n",
    "\n",
    "* Since that paper, people have used the transformer architecture in many different ways, for many different tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Just the encoder ([BERT](https://arxiv.org/abs/1810.04805))\n",
    "  <center><img src=\"./assets/attn_bert.png\" style=\"height:200px\"></center>\n",
    "  * This is just a feature extractor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  * So we can add heads for specific tasks.\n",
    "    * Can be used with pre-training and fine-tuning.\n",
    "  <center><img src=\"./assets/attn_bert_classifier.png\" style=\"height:300px\"></center>\n",
    "\n",
    "* [Image Source: Jay Alammar's The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Just the decoder ([GPT-2](https://openai.com/blog/better-language-models/))\n",
    "  <center><img src=\"./assets/attn_gpt_2.gif\" style=\"height:300px\"></center>\n",
    "  \n",
    "  * Generate data - for many types of tasks.\n",
    "* [GIF adapted from: Jay Alammar's The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Why not use them on everything, everywhere, all at once?\n",
    "<center><img src=\"./assets/attn_chatgpt.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "\n",
    "* Both of the channels below are very recommended for all things deep learning\n",
    "  * <a href=\"https://www.youtube.com/watch?v=iDulhoQ2pro\"> Yannic Kilcher - Attention Is All You Need</a>\n",
    "  * <a href=\"https://www.youtube.com/watch?v=cbYxHkgkSVs\">Aleksa Gordić - The AI Epiphany - Attention Is All You Need (Transformer) | Paper Explained\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* ECE 046211 Winter 22-23 - <a href=\"https://taldatech.github.io/\">Tal Daniel</a> \n",
    "* CS 236781 Winter 22-23 - <a href=\"https://vistalab-technion.github.io/cs236781/tutorials/\">Moshe Kimhi, Aviv A. Rosenberg</a> \n",
    "* <a href=\"https://jalammar.github.io/illustrated-transformer/\">Jay Alammar's The Illustrated Transformer</a>\n",
    "* <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">Jay Alammar's Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a>\n",
    "* [Attention is all you need, Ashish Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "rise": {
   "scroll": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
