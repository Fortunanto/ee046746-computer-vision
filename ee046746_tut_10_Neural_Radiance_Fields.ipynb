{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "---\n",
    "#### <a href=\"https://taldatech.github.io/\"></a> \n",
    "\n",
    "\n",
    "## Tutorial 10 - Neural Radiance Fields\n",
    "---\n",
    "\n",
    "<img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png\" style=\"height:300px\">\n",
    "\n",
    "* <a href=\"https://www.matthewtancik.com/nerf\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [What is NeRF](#-What-is-NeRF)\n",
    "<!--     * [Image Classifcation + Object Localization = Object Detection](#-Image-Classifcation-+-Object-Localization-=-Object-Detction)\n",
    "    * [Localization Approaches](#-Localiztion-Approaches)\n",
    "        * Sliding Windows Approach\n",
    "    * [Performance Metrics](#-Performance-Metrics) -->\n",
    "* [Components to NeRF](#-TODO)\n",
    "    * [What is Implicit Neural Representations](#-What-is-Implicit-Neural-Representations)\n",
    "    * [Camera Localization](#-TODO)\n",
    "    * [Positional Encoding](#-TODO)\n",
    "    * [Ray Casting](#-TODO)\n",
    "    * [Coarse to Fine Representation](#-TODO)\n",
    "    * [Volume Rendering](#-TODO)\n",
    "    * [Model Architecture](#-TODO)\n",
    "* [Extensions](#-TODO)\n",
    "    * [Performance](#-TODO?)\n",
    "    * [Temporal NeRF](#-TODO)\n",
    "    * [Others](#-TODO)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/color/96/null/view-details.png\" style=\"height:50px;display:inline\"> What is NeRF\n",
    "---\n",
    "- NeRF (Neural Radiance Fields) is a technique that uses deep neural networks to generate 3D reconstructions and render novel views of a scene.\n",
    "- NeRF learns a continuous representation of the scene's appearance and geometry directly from 2D images, and their estimated camera locations, without relying on sparse feature matches or point correspondences.\n",
    "- It represents the scene as a volumetric function, estimating radiance (color) at any 3D point within the scene. this representation is known as an Implicit Neural Representation\n",
    "- NeRF captures fine surface details, complex lighting effects, reflections, and refractions, producing highly realistic renderings.\n",
    "- To train NeRF, a set of images taken from different viewpoints is used as input to the neural network.\n",
    "- During training, NeRF learns to predict the volumetric representation by minimizing the discrepancy between predicted and ground truth images.\n",
    "- Once trained, NeRF can generate novel views of the scene from any desired camera viewpoint, even if it was not present in the training data.\n",
    "![NeRF-Drums.png](assets/NeRF-Drums.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "* Essentially, we \"launch\" rays from each 3D Camera location, and attempt to \"predict\" the density and RGB color along the ray\n",
    "![nerf](assets/NeRF_scheme.png)\n",
    "* The density and color prediction is carried out by an \"Implicit Neural Representation\"\n",
    "* We'll now see how each part integrates into NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/plus.png\" style=\"height:50px;display:inline\"> What is Implicit Neural Representations\n",
    "---\n",
    "- As said before, an implicit neural representation represents various types of data using deep neural networks(Images NeRFS and even other neural networks!).\n",
    "- In 3D, we can look at explicit, or implicit representations.\n",
    "- Explicit representations stores explicit geometric information like vertices and voxels, implicit representations model the surface or volume as an implicit function.\n",
    "- When representing 3D data, it can represent a wide range of shapes, including objects with holes, handles, and self-intersections.\n",
    "- They have gained popularity due to their ability to handle complex and diverse shapes and their potential for end-to-end learning.\n",
    "\n",
    "<img src=\"./assets/ImplicitRep.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/100/000000/google-maps.png\" style=\"height:50px;display:inline\"> Camera Localization\n",
    "---\n",
    "- In order to launch rays from the cameras, we must first know, \"where\" they are against the scene\n",
    "- Accurate camera poses are necessary for NeRF to correctly align the images and estimate the scene's geometry and appearance.\n",
    "- One common approach for camera localization is using a structure-from-motion (SfM) pipeline like Colmap. as you know, SfM algorithms analyze image correspondences and geometric constraints to estimate camera poses and sparse 3D points.\n",
    "- Colmap, for instance, can automatically detect feature correspondences across images, perform bundle adjustment to refine camera poses and 3D point locations, and provide accurate camera localization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These techniques have some difficulties with scenes with smooth or repetitive textures.\n",
    "- Synthetic datasets, created through computer graphics techniques, can provide known camera locations. These datasets are often used to train and evaluate NeRF models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Positional Encoding\n",
    "---\n",
    "- The very same concept from transformers\n",
    "- In the context of NeRF, positional encoding is used to encode the 3D spatial coordinates of points in the scene.\n",
    "- By incorporating positional encoding, NeRF allows the network to learn a higher frequency representation of the scene, thus allowing for capturing of greater detail.\n",
    "- This helps NeRF capture complex geometric structures and relationships, enabling accurate reconstruction and rendering.\n",
    "- The positional encoding in NeRF works in conjunction with other network layers that capture appearance and radiance information.\n",
    "![Positional_Encoding.png](assets/Positional_Encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Casting\n",
    "---\n",
    "- Similarly to triangulation, we launch virtual rays from each camera location.\n",
    "- Each ray is defined by its origin (the camera viewpoint) and a direction (pointing towards a pixel on the image plane).\n",
    "- Along each ray, the radiance values are estimated by querying the neural network representing the volumetric function.\n",
    "- This estimation involves sampling points along the ray and evaluating the neural network at those points to obtain radiance values - Computationally Expensive!\n",
    "- The quality of the rendered views depends on factors like image resolution, the number of rays per pixel, and the accuracy of radiance estimation(Hyper Parameters)\n",
    "\n",
    "![Pinhole camera](https://www.researchgate.net/profile/Willy-Azarcoya-Cabiedes/publication/317498100/figure/fig10/AS:610418494013440@1522546518034/Pin-hole-camera-model-terminology-The-optical-center-pinhole-is-placed-at-the-origin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Sampling\n",
    "\n",
    "Now that we've understood how to cast rays, we need to understand how to \"discretize\" our density function.\n",
    "For that, we will take the idea of \"sampling\" along the ray we've cast. \n",
    "The stratified sampling approach splits the ray into evenly-spaced bins and randomly samples within each bin. The perturb setting determines whether to sample points uniformly from each bin or to simply use the bin center as the point. In most cases, We will perturb the samples. as it will encourage the network to learn over a continuously sampled space.\n",
    "\n",
    "![stratified_sampling.png](assets/stratified_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Volume Sampling\n",
    "\n",
    "The 3D space is in fact very sparse with occlusions and so most points don't contribute much to the rendered image. It is therefore more beneficial to oversample regions with a high likelihood of contributing to the integral.\n",
    "\n",
    "This is done by applying a learned weighting scheme to the first, *coarse* set of samples, to create a PDF across the whole ray, then sampling from that PDF to generate a finer sample of points, that will be forwarded through a another, fine version of the NeRF.\n",
    "![hierarchical_sampling.png](assets/hierarchical_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Rendering\n",
    "\n",
    "![nerf](assets/NeRF_scheme.png)\n",
    "\n",
    "Putting it all together, actually rendering the image follows the following formula:\n",
    " \n",
    "$$C(r)=\\int_{t_n}^{t_f} T(t)\\sigma(r(t))c(r(t),d)  dt \\quad \\text{Where} \\quad T(t)=exp\\big(-\\int_{t_n}^{t} \\sigma(r(s)) ds\\big) $$ And $$ \\textbf{r}(t)=o+t\\textbf{d} $$ \n",
    "\n",
    "Where we approximate the integral with the quadrature rule, like so\n",
    "\n",
    "$$\n",
    "\\hat{C}(r) = \\sum_{i=1}^{N}  \\left(1 - \\exp\\left(-\\sigma_i \\delta_i\\right)\\right) c_i, \\quad \\text{where} \\quad T_i = \\exp\\left(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j\\right)\n",
    "$$\n",
    "\n",
    "Therefore, per image pixel we \"launch\" a ray originating from the camera center, going through the point, and calculate the Transmitance, Color, and Density of the volume point encountered through the ray.\n",
    "\n",
    "As specified before, there can be both a rendering through the \"coarse\" NeRF, and through the \"fine\" one, which is usually output as the output novel view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finishing up\n",
    "---\n",
    "\n",
    "Now that we can render an entire new, novel view, we go to the obvious question\n",
    "\n",
    "\"How do you even train this?!\"\n",
    "\n",
    "Good Question!\n",
    "\n",
    "all that it takes is a simple, L2 Loss, on the reconstruction of a known, given image! \n",
    "\n",
    "In order to force both the coarse model, and the fine model correspond to the expected image, the loss goes as follows:\n",
    "\n",
    "$$\n",
    "L = \\sum_{r \\in R} \\left( \\lVert C_{c}(r) - C(r)\\rVert^2+ \\lVert C_{f}(r) - C(r)\\rVert^2 \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What came after\n",
    "\n",
    "NeRF blew the door open for many other works which followed up on the tecnique presented by NeRF, we'll show 3, that each touch up on a different issue the paper didn't solve completly\n",
    "nerf\n",
    "![nerf-scholar.png](assets/nerf-scholar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Rendering Speed\n",
    "# Instant Neural Graphics Primitives with Multiresolution Hashed Neural Networks\n",
    "\n",
    "- As said before, implicit representations are very good for representing dense, high resolution information(Giga pixel images, videos, and NeRFs) but are very very (very very ....) slow to evaluate and train.\n",
    " - This paper introduces the idea of using some sort of \"hybrid\" representation, where we can combine the best of both worlds, both the speed of rendering of explicit representations, and the high quality recreation of the implicit ones.\n",
    "- Specifically, multiresolution hierarchy of hash tables which provides adaptivity and efficiency:\n",
    "  - Adaptivity: The method maps a cascade of grids to corresponding fixed-size arrays of feature vectors.\n",
    "      - At coarse resolutions, there is a 1:1 mapping from grid points to array entries.\n",
    "      - At fine resolutions, the array is treated as a hash table and indexed using a spatial hash function, where multiple grid points alias each array entry. The hash tables automatically prioritize the sparse areas with the most important fine-scale detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specifically, multiresolution hierarchy of hash tables which provides adaptivity and efficiency:\n",
    "  - Adaptivity: The method maps a cascade of grids to corresponding fixed-size arrays of feature vectors.\n",
    "      - At coarse resolutions, there is a 1:1 mapping from grid points to array entries.\n",
    "      - At fine resolutions, the array is treated as a hash table and indexed using a spatial hash function, where multiple grid points alias each array entry. The hash tables automatically prioritize the sparse areas with the most important fine-scale detail.\n",
    "  - Efficiency: The hash table lookups are efficient and the method has been validated in four representative tasks: learning the mapping from 2D coordinates to RGB colors of a high-resolution image, learning the mapping from 3D coordinates to the distance to a surface, learning the 5D light field of a given scene from a Monte Carlo path tracer, and learning the 3D density and 5D light field of a given scene from image observations and corresponding perspective transforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The paper combines the ideas of parametric encoding and spatial hashing to reduce waste. The trainable feature vectors are stored in a compact spatial hash table, whose size is a hyperparameter. The method uses multiple separate hash tables indexed at different resolutions, whose interpolated outputs are concatenated before being passed through the MLP. The neural network learns to disambiguate hash collisions itself, avoiding control flow divergence, reducing implementation complexity, and improving performance.\n",
    "\n",
    "<video height=\"600\" width=\"900\" controls src=\"https://nvlabs.github.io/instant-ngp/assets/nerf_grid_lq.mp4\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-NeRF\n",
    "Introduces the following\n",
    "\n",
    "1. **Dynamic Scene Rendering**: D-NeRF represents the first end-to-end neural rendering system that is applicable to dynamic scenes, encompassing both static and moving/deforming objects. This is a significant innovation, as it requires only a single camera, does not necessitate pre-computed 3D reconstruction, and can be trained end-to-end.\n",
    "\n",
    "2. **Two-stage Learning Process**: The paper introduces a two-stage learning process. The first stage encodes the scene into a canonical space, and the second maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Post training, D-NeRF can render novel images, controlling both the camera view and the time variable, thereby manipulating object movement.\n",
    "\n",
    "3. **Time Component in 6D Function**: The authors propose a continuous 6D function to represent the input of the system. This function considers 3D location, camera view, and importantly, a time component. By incorporating time as an essential variable, the model can handle dynamic scene changes effectively.\n",
    "\n",
    "5. **3D Mesh Production**: An interesting side product of the D-NeRF approach is its ability to generate complete 3D meshes capturing the time-varying geometry of scenes. This capability is remarkable because these meshes are produced by observing the scene under a specific deformation only from one single viewpoint.\n",
    "\n",
    "This is the first paper pertaining to Dynamic NeRFs(Very cool!)\n",
    "![d_NeRF.png](assets/d_NeRF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video height=\"600\" width=\"900\" controls src=\"https://www.albertpumarola.com/images/2021/D-NeRF/standup.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DyNeRF\n",
    "\n",
    "1. **Dynamic Neural Radiance Field:**\n",
    "    - The authors extend neural radiance fields to the space-time domain.\n",
    "    - Instead of directly using time as input, they parameterize scene motion and appearance changes by a set of compact latent codes.\n",
    "    - These learned latent codes show more expressive power, allowing for recording the vivid details of moving geometry and texture.\n",
    "    - They also allow for smooth interpolation in time, which enables visual effects such as slow motion or ‘bullet time’.\n",
    "    \n",
    "![DyNeRF.png](assets/DyNeRF.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Novel Importance Sampling Strategies:**\n",
    "    - Captured dynamic video often exhibits a small amount of pixel change between frames, providing an opportunity to significantly boost the training progress by selecting the pixels that are most important for training.\n",
    "    - In the time dimension, they schedule training with coarse-to-fine hierarchical sampling in the frames. In the ray/pixel dimension, their design tends to sample those pixels that are more time-variant than others.\n",
    "    - The resulting representation is very small comparitvely to the amount of information stored (28 MB for a 10 second, 30 fps, 18 camera setup)\n",
    "![temporal_hierarchical_sampling.png](assets/temporal_hierarchical_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And many more\n",
    "\n",
    "this was just a partial overview of the papers that expand on nerf\n",
    "\n",
    "a more complete list can be found here https://github.com/awesome-NeRF/awesome-NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it yourself at home\n",
    "\n",
    "all of the code necessary to train nerf is widely available, either at https://github.com/yenchenlin/nerf-pytorch or, using the NeRF-Studio Suite\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nerfstudio.png](assets/nerfstudio.png)\n",
    "\n",
    "Nerfstudio is a Python library that provides a simplified end-to-end process for creating, training, and visualizing Neural Radiance Fields (NeRFs). The library aims to make NeRFs more interpretable by modularizing each component. Additionally, it provides learning resources to help users understand and keep up-to-date with NeRF technology. Nerfstudio encourages contributions from its users, whether it's a feature request, a new NeRF model, or a new dataset.\n",
    "\n",
    "### Supported Methods\n",
    "\n",
    "Included Methods:\n",
    "\n",
    "- Nerfacto: Recommended method, integrates multiple methods into one.\n",
    "- Instant-NGP: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\n",
    "- NeRF: OG Neural Radiance Fields\n",
    "- Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\n",
    "- TensoRF: Tensorial Radiance Fields\n",
    "\n",
    "Third-party Methods:\n",
    "\n",
    "- Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions\n",
    "- K-Planes: Unified 3D and 4D Radiance Fields\n",
    "- LERF: Language Embedded Radiance Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "* (Deep) Object Detection - <a href=\"https://www.youtube.com/watch?v=nDPWywWRIRo\"> Stanford CS231 - Lecture 11 | Detection and Segmentation</a>\n",
    "* Object Detection - <a href=\"https://www.youtube.com/watch?v=5e5pjeojznk&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=26&t=0s\">C4W3L03 Object Detection - Andrew Ng</a>\n",
    "* Non-Maximum Supression - <a href=\"https://www.youtube.com/watch?v=VAo84c1hQX8&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=29&t=0s\">C4W3L07 Nonmax Suppression - Andrew Ng</a>\n",
    "* Region Proposals - <a href=\"https://www.youtube.com/watch?v=6ykvU9WuIws&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=32&t=0s\">C4W3L10 Region Proposals - Andrew Ng</a>\n",
    "* R-CNN, Fast R-CNN, Faster R-CNN - <a href=\"https://www.youtube.com/watch?v=a9_8wqMxVkY\">RCNN, FAST RCNN, FASTER RCNN : OBJECT DETECTION AND LOCALIZATION THROUGH DEEP NEURAL NETWORKS</a>\n",
    "* YOLO - <a href=\"https://www.youtube.com/watch?v=9s_FpMpdYW8&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=31&t=0s\">C4W3L09 YOLO Algorithm - Andrew Ng</a>\n",
    "* YOLO v3 - <a href=\"https://www.youtube.com/watch?time_continue=1&v=MPU2HistivI&feature=emb_logo\">YOLOv3</a>\n",
    "* YOLO v4 - <a href=\"https://www.youtube.com/watch?v=_JzOFWx1vZg\">Yolo V4 - How it Works and Why it's So Amazing!</a>\n",
    "* NeRF Studio - <a href=\"https://www.youtube.com/watch?v=nSFsugarWzk\">Getting Started</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* EE 046746 Spring 21 - <a href=\"https://taldatech.github.io/\">Tal Daniel</a> \n",
    "* <a href=\"https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/\">Zero to Hero: Guide to Object Detection using Deep Learning: Faster R-CNN,YOLO,SSDO - Ankit Sachan</a>\n",
    "* <a href=\"https://machinelearningmastery.com/object-recognition-with-deep-learning/\">A Gentle Introduction to Object Recognition With Deep Learning - Jason Brownlee</a>\n",
    "* <a href=\"https://developers.arcgis.com/python/guide/how-ssd-works/\">\n",
    "How single-shot detector (SSD) works?</a>\n",
    "* <a href=\"https://towardsdatascience.com/whats-new-in-yolov4-323364bb3ad3\">What’s new in YOLOv4?</a>\n",
    "* <a href=\"https://heartbeat.fritz.ai/introduction-to-yolov4-research-review-5b6b4bd5f255\">Introduction to YOLOv4: Research review</a>\n",
    "* Slides by David Dov and Yael Amiay.\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
